{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used in the https://colab.research.google.com/ Ram:0.81GB/12.72GB\n",
    "\n",
    "range 200000\n",
    "\n",
    "use numpy = 79.903 ms\n",
    "use tensor = 35.394 ms\n",
    "\n",
    "tensor is very faster than numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start cod\n",
    "import torch\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#introduction dtype ans device\n",
    "dty = torch.float\n",
    "dev = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input & output data\n",
    "x = torch.linspace(-math.pi, math.pi, 2000, device=dev, dtype=dty)\n",
    "y = torch.sin(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random weights\n",
    "a = torch.randn((), device=dev, dtype=dty)\n",
    "b = torch.randn((), device=dev, dtype=dty)\n",
    "c = torch.randn((), device=dev, dtype=dty)\n",
    "d = torch.randn((), device=dev, dtype=dty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 816579328.0\n",
      "199 170710640.0\n",
      "299 90696856.0\n",
      "399 142071760.0\n",
      "499 229001504.0\n",
      "599 333133216.0\n",
      "699 452474592.0\n",
      "799 587767936.0\n",
      "899 739801152.0\n",
      "999 909066112.0\n",
      "1099 1095831168.0\n",
      "1199 1300234752.0\n",
      "1299 1522347008.0\n",
      "1399 1762202368.0\n",
      "1499 2019818752.0\n",
      "1599 2295204352.0\n",
      "1699 2588364544.0\n",
      "1799 2899297536.0\n",
      "1899 3228008448.0\n",
      "1999 3574496768.0\n",
      "2099 3938763264.0\n",
      "2199 4320807424.0\n",
      "2299 4720629248.0\n",
      "2399 5138229248.0\n",
      "2499 5573606400.0\n",
      "2599 6026761728.0\n",
      "2699 6497695232.0\n",
      "2799 6986405888.0\n",
      "2899 7492895232.0\n",
      "2999 8017161216.0\n",
      "3099 8559205376.0\n",
      "3199 9119027200.0\n",
      "3299 9696626688.0\n",
      "3399 10292003840.0\n",
      "3499 10905159680.0\n",
      "3599 11536091136.0\n",
      "3699 12184802304.0\n",
      "3799 12851290112.0\n",
      "3899 13535555584.0\n",
      "3999 14237600768.0\n",
      "4099 14957421568.0\n",
      "4199 15695022080.0\n",
      "4299 16450398208.0\n",
      "4399 17223553024.0\n",
      "4499 18014486528.0\n",
      "4599 18823194624.0\n",
      "4699 19649685504.0\n",
      "4799 20493950976.0\n",
      "4899 21355995136.0\n",
      "4999 22235815936.0\n",
      "5099 23133417472.0\n",
      "5199 24048793600.0\n",
      "5299 24981948416.0\n",
      "5399 25932881920.0\n",
      "5499 26901592064.0\n",
      "5599 27888080896.0\n",
      "5699 28892348416.0\n",
      "5799 29914390528.0\n",
      "5899 30954213376.0\n",
      "5999 32011810816.0\n",
      "6099 33087188992.0\n",
      "6199 34180345856.0\n",
      "6299 35291279360.0\n",
      "6399 36419989504.0\n",
      "6499 37566476288.0\n",
      "6599 38730743808.0\n",
      "6699 39912783872.0\n",
      "6799 41112608768.0\n",
      "6899 42330206208.0\n",
      "6999 43565580288.0\n",
      "7099 44818739200.0\n",
      "7199 46089670656.0\n",
      "7299 47378382848.0\n",
      "7399 48684867584.0\n",
      "7499 50009137152.0\n",
      "7599 51351175168.0\n",
      "7699 52711002112.0\n",
      "7799 54088597504.0\n",
      "7899 55483977728.0\n",
      "7999 56897134592.0\n",
      "8099 58328064000.0\n",
      "8199 59776770048.0\n",
      "8299 61243260928.0\n",
      "8399 62727528448.0\n",
      "8499 64229576704.0\n",
      "8599 65749393408.0\n",
      "8699 67286990848.0\n",
      "8799 68842373120.0\n",
      "8899 70415515648.0\n",
      "8999 72006459392.0\n",
      "9099 73615163392.0\n",
      "9199 75241652224.0\n",
      "9299 76885917696.0\n",
      "9399 78547959808.0\n",
      "9499 80227778560.0\n",
      "9599 81925382144.0\n",
      "9699 83640770560.0\n",
      "9799 85373911040.0\n",
      "9899 87124844544.0\n",
      "9999 88893554688.0\n",
      "10099 90680041472.0\n",
      "10199 92484304896.0\n",
      "10299 94306353152.0\n",
      "10399 96146178048.0\n",
      "10499 98003779584.0\n",
      "10599 99879149568.0\n",
      "10699 101772304384.0\n",
      "10799 103683235840.0\n",
      "10899 105611952128.0\n",
      "10999 107558436864.0\n",
      "11099 109522714624.0\n",
      "11199 111504744448.0\n",
      "11299 113504567296.0\n",
      "11399 115522174976.0\n",
      "11499 117557542912.0\n",
      "11599 119610703872.0\n",
      "11699 121681633280.0\n",
      "11799 123770347520.0\n",
      "11899 125876822016.0\n",
      "11999 128001097728.0\n",
      "12099 130143133696.0\n",
      "12199 132302979072.0\n",
      "12299 134480551936.0\n",
      "12399 136675934208.0\n",
      "12499 138889084928.0\n",
      "12599 141120028672.0\n",
      "12699 143368749056.0\n",
      "12799 145635229696.0\n",
      "12899 147919486976.0\n",
      "12999 150221537280.0\n",
      "13099 152541347840.0\n",
      "13199 154878951424.0\n",
      "13299 157234331648.0\n",
      "13399 159607488512.0\n",
      "13499 161998405632.0\n",
      "13599 164407132160.0\n",
      "13699 166833618944.0\n",
      "13799 169277865984.0\n",
      "13899 171739906048.0\n",
      "13999 174219739136.0\n",
      "14099 176717348864.0\n",
      "14199 179232718848.0\n",
      "14299 181765865472.0\n",
      "14399 184316805120.0\n",
      "14499 186885505024.0\n",
      "14599 189472014336.0\n",
      "14699 192076267520.0\n",
      "14799 194698313728.0\n",
      "14899 197338120192.0\n",
      "14999 199995736064.0\n",
      "15099 202671095808.0\n",
      "15199 205364264960.0\n",
      "15299 208075194368.0\n",
      "15399 210803900416.0\n",
      "15499 213550383104.0\n",
      "15599 216314675200.0\n",
      "15699 219096694784.0\n",
      "15799 221896491008.0\n",
      "15899 224714113024.0\n",
      "15999 227549495296.0\n",
      "16099 230402637824.0\n",
      "16199 233273589760.0\n",
      "16299 236162285568.0\n",
      "16399 239068790784.0\n",
      "16499 241993039872.0\n",
      "16599 244935065600.0\n",
      "16699 247894884352.0\n",
      "16799 250872496128.0\n",
      "16899 253867884544.0\n",
      "16999 256881016832.0\n",
      "17099 259911942144.0\n",
      "17199 262960693248.0\n",
      "17299 266027171840.0\n",
      "17399 269111427072.0\n",
      "17499 272213475328.0\n",
      "17599 275333283840.0\n",
      "17699 278470885376.0\n",
      "17799 281626279936.0\n",
      "17899 284799401984.0\n",
      "17999 287990349824.0\n",
      "18099 291199057920.0\n",
      "18199 294425559040.0\n",
      "18299 297669820416.0\n",
      "18399 300931874816.0\n",
      "18499 304211689472.0\n",
      "18599 307509264384.0\n",
      "18699 310824665088.0\n",
      "18799 314157793280.0\n",
      "18899 317508747264.0\n",
      "18999 320877428736.0\n",
      "19099 324263968768.0\n",
      "19199 327668203520.0\n",
      "19299 331090231296.0\n",
      "19399 334530084864.0\n",
      "19499 337987665920.0\n",
      "19599 341463040000.0\n",
      "19699 344956207104.0\n",
      "19799 348467101696.0\n",
      "19899 351995822080.0\n",
      "19999 355542335488.0\n",
      "tensor(19995.7715)\n"
     ]
    }
   ],
   "source": [
    "#l = learning rate\n",
    "l = 1e-7\n",
    "for a in range(20000):\n",
    "    y_second = a + b*x + c*x**2 + d*x** 3\n",
    "    \n",
    "    #compute loss * s=loss\n",
    "    s = (y_second - y).pow(2).sum().item()\n",
    "    if a % 100 == 99:\n",
    "        print(a,s)\n",
    "        \n",
    "    #g = gradients \n",
    "    g_y_second = 1.8 * (y_second - y)\n",
    "    g_a = g_y_second.sum()\n",
    "    g_b = (g_y_second * x).sum()\n",
    "    g_c = (g_y_second * x**2).sum()\n",
    "    g_d = (g_y_second * x**3).sum()\n",
    "    \n",
    "    #weights gradient\n",
    "    a -= l * g_a\n",
    "    b -= l * g_b\n",
    "    c -= l * g_c\n",
    "    d -= l * g_d\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: y = 19995.771484375 + 0.8019372224807739 x + -3350.00927734375 x^2 + -0.0855354517698288 x^3\n"
     ]
    }
   ],
   "source": [
    "print(f'Result: y = {a.item()} + {b.item()} x + {c.item()} x^2 + {d.item()} x^3')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
